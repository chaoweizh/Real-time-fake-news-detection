{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc139adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.clustering import KMeans,KMeansModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from collections import Counter\n",
    "import math\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadfac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    all_TFIDF = []\n",
    "    for tfBow in all_TF:\n",
    "        tfidf = {}\n",
    "        for word, val in tfBow.items():\n",
    "            tfidf[word] = val*idfs[word]\n",
    "        all_TFIDF.append(tfidf)\n",
    "    return all_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d84826",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = defaultdict(list)\n",
    "with open(\"/home/hadoop/Spark-Programs/csv/topics.csv\", \"r\") as sentences_file:\n",
    "    reader = csv.reader(sentences_file, delimiter=',')\n",
    "    for row in reader:\n",
    "        episodes[row[0]] = (','.join(row[1:]))  \n",
    "        \n",
    "counted_topics_set = []\n",
    "all_topics_sets = {}\n",
    "for topics in episodes.values():\n",
    "    all_topics_sets = set(topics_sets).union(set(topics.split(',')))\n",
    "    counted_topics = Counter(topics.split(','))\n",
    "    counted_topics_set.append(dict(counted_topics)) \n",
    "intial_vectors = []\n",
    "all_TF = []\n",
    "for files in counted_topics_set:\n",
    "    intial_vector = dict.fromkeys(all_topics_sets, 0)\n",
    "    for topic in files:\n",
    "        intial_vector[topic]+=files[topic]\n",
    "    TF = computeTF(intial_vector,files.keys())\n",
    "    intial_vectors.append(intial_vector)\n",
    "    all_TF.append(TF)\n",
    "\n",
    "IDFs = computeIDF(intial_vectors)\n",
    "\n",
    "TFIDFs = computeTFIDF(all_TF,IDFs)\n",
    "\n",
    "topics_Dict = (pd.DataFrame(TFIDFs))\n",
    "np.savetxt('/home/hadoop/output.txt', topics_Dict.values, delimiter=\",\", fmt=\"%s\") \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    rel = {}\n",
    "    print(type(x))\n",
    "    rel['features'] = Vectors.dense([float(a) for a in x])\n",
    "    return rel \n",
    "\n",
    "df = sc.textFile(\"/home/hadoop/output\").map(lambda line: line.split(','))\n",
    "\n",
    "df = df.map(lambda p: Row(**f(p))).toDF()\n",
    "\n",
    "kmeansmodel = KMeans().setK(6).setFeaturesCol('features')\n",
    "kmeansmodel = kmeansmodel.setPredictionCol('prediction')\n",
    "kmeansmodel = kmeansmodel.fit(df)\n",
    "results = kmeansmodel.transform(df).collect()\n",
    "counter = 1\n",
    "for item in results:\n",
    "    print('No.'+ str(counter) +' data'+' is predcted as cluster'+ str(item[1]))\n",
    "    counter+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8a4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d522f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fbd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cba99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
